{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded\n"
     ]
    }
   ],
   "source": [
    "if pathlib.Path('dredze_amazon_reviews.zip').exists():\n",
    "    print(\"Already downloaded\")\n",
    "else:\n",
    "    !wget http://www.cse.chalmers.se/~richajo/waspnlp2024/dredze_amazon_reviews.zip\n",
    "    !unzip dredze_amazon_reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model_name):\n",
    "        super().__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "    def forward(self, Xbatch, Xmask):\n",
    "        ber_out = self.bert_model(input_ids=Xbatch, attention_mask=Xmask)\n",
    "        return ber_out.last_hidden_state[:, 0, :]\n",
    "    \n",
    "class DocumentBatcher:\n",
    "    \"\"\"A collator that builds a batch from a number of documents.\"\"\"\n",
    "\n",
    "    def __init__(self, pad_id):\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "    def make_batch_1(self, X):\n",
    "        \"\"\"Build a batch from a number of documents.\n",
    "        Returns a tensor of shape [n_docs, max_doc_length].\"\"\"\n",
    "\n",
    "        # How long is the longest document in this batch?\n",
    "        max_len = max(len(x) for x in X)\n",
    "\n",
    "        # Build the document tensor. We pad the shorter documents so that all documents\n",
    "        # have the same length.\n",
    "        Xpadded = torch.as_tensor([x + [self.pad_id]*(max_len-len(x)) for x in X])\n",
    "        return Xpadded\n",
    "\n",
    "\n",
    "    def make_batch_2(self, XY):\n",
    "        \"\"\"Build a batch from a number of documents and their labels.\n",
    "        Returns two tensors X and Y, where X is the document tensor,\n",
    "        of shape [n_docs, max_doc_length]\n",
    "\n",
    "        and\n",
    "\n",
    "        Y is the label tensor, of shape [n_docs].\n",
    "        \"\"\"\n",
    "\n",
    "        # How long is the longest document in this batch?\n",
    "        max_len = max(len(x) for x, _ in XY)\n",
    "\n",
    "        # Build the document tensor. We pad the shorter documents so that all documents\n",
    "        # have the same length.\n",
    "        Xpadded = torch.as_tensor([x + [self.pad_id]*(max_len-len(x)) for x, _ in XY])\n",
    "\n",
    "        # Build the label tensor.\n",
    "        Y = torch.as_tensor([y for _, y in XY])\n",
    "\n",
    "        return Xpadded, Y\n",
    "\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        if isinstance(instances[0], tuple):\n",
    "            return self.make_batch_2(instances)\n",
    "        else:\n",
    "            return self.make_batch_1(instances)\n",
    "        \n",
    "def encoder_loader(data, tokenizer, max_length=512, batch_size=32):\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = list(data)\n",
    "        data_encoded = tokenizer(data, truncation=True, max_length=max_length).input_ids\n",
    "        batcher = DocumentBatcher(tokenizer.pad_token_id)\n",
    "        data_index = list(zip(data_encoded, range(len(data_encoded))))\n",
    "        data_loader = DataLoader(data_index, batch_size, collate_fn=batcher)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_corpus = pd.read_csv('dredze_amazon_reviews.tsv', sep='\\t', header=None, names=['product', 'sentiment', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertEncoder(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_amazon_corpus = amazon_corpus.copy()\n",
    "emb_amazon_corpus['text_embedding'] = None\n",
    "bert_model.eval()\n",
    "bert_model.to('cuda')\n",
    "with torch.no_grad():\n",
    "    for batch, ids in encoder_loader(amazon_corpus.text, tokenizer):\n",
    "        mask = (batch != tokenizer.pad_token_id).long().to('cuda')\n",
    "        batch = batch.to('cuda')\n",
    "        bert_output = bert_model(batch, mask)\n",
    "        for i, idx in enumerate(ids):\n",
    "            emb_amazon_corpus.at[idx.item(), 'text_embedding'] = bert_output[i].cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>music</td>\n",
       "      <td>neg</td>\n",
       "      <td>i bought this album because i loved the title ...</td>\n",
       "      <td>[0.17977853, -0.18256074, -0.3078683, 0.090951...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>music</td>\n",
       "      <td>neg</td>\n",
       "      <td>i was misled and thought i was buying the enti...</td>\n",
       "      <td>[-0.08570315, -0.16243924, 0.14063138, 0.26266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>books</td>\n",
       "      <td>neg</td>\n",
       "      <td>i have introduced many of my ell , high school...</td>\n",
       "      <td>[-0.042635042, -0.29660478, 0.17880535, -0.525...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>books</td>\n",
       "      <td>pos</td>\n",
       "      <td>anything you purchase in the left behind serie...</td>\n",
       "      <td>[0.18510504, 0.033130858, 0.17519405, -0.29427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dvd</td>\n",
       "      <td>pos</td>\n",
       "      <td>i loved these movies , and i cant wiat for the...</td>\n",
       "      <td>[-0.011192498, -0.039981, -0.025789035, -0.035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11909</th>\n",
       "      <td>dvd</td>\n",
       "      <td>neg</td>\n",
       "      <td>the story here dose n't matter . the main char...</td>\n",
       "      <td>[0.08249482, 0.30193207, -0.0017076576, 0.0646...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11910</th>\n",
       "      <td>software</td>\n",
       "      <td>pos</td>\n",
       "      <td>i liked everything about this product except i...</td>\n",
       "      <td>[0.014326348, -0.3186368, 0.26025128, -0.46153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11911</th>\n",
       "      <td>camera</td>\n",
       "      <td>pos</td>\n",
       "      <td>this flash is the perfect back-up for a studio...</td>\n",
       "      <td>[-0.23396452, -0.028989198, 0.41473043, -0.179...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11912</th>\n",
       "      <td>health</td>\n",
       "      <td>neg</td>\n",
       "      <td>i had boughten this as a gift which turned out...</td>\n",
       "      <td>[0.22810979, 0.027687859, 0.12501115, -0.54732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11913</th>\n",
       "      <td>health</td>\n",
       "      <td>neg</td>\n",
       "      <td>the pedometer arrive held prisoner in a diffic...</td>\n",
       "      <td>[-0.41367865, 0.10774601, 0.24124433, -0.22921...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11914 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        product sentiment                                               text  \\\n",
       "0         music       neg  i bought this album because i loved the title ...   \n",
       "1         music       neg  i was misled and thought i was buying the enti...   \n",
       "2         books       neg  i have introduced many of my ell , high school...   \n",
       "3         books       pos  anything you purchase in the left behind serie...   \n",
       "4           dvd       pos  i loved these movies , and i cant wiat for the...   \n",
       "...         ...       ...                                                ...   \n",
       "11909       dvd       neg  the story here dose n't matter . the main char...   \n",
       "11910  software       pos  i liked everything about this product except i...   \n",
       "11911    camera       pos  this flash is the perfect back-up for a studio...   \n",
       "11912    health       neg  i had boughten this as a gift which turned out...   \n",
       "11913    health       neg  the pedometer arrive held prisoner in a diffic...   \n",
       "\n",
       "                                          text_embedding  \n",
       "0      [0.17977853, -0.18256074, -0.3078683, 0.090951...  \n",
       "1      [-0.08570315, -0.16243924, 0.14063138, 0.26266...  \n",
       "2      [-0.042635042, -0.29660478, 0.17880535, -0.525...  \n",
       "3      [0.18510504, 0.033130858, 0.17519405, -0.29427...  \n",
       "4      [-0.011192498, -0.039981, -0.025789035, -0.035...  \n",
       "...                                                  ...  \n",
       "11909  [0.08249482, 0.30193207, -0.0017076576, 0.0646...  \n",
       "11910  [0.014326348, -0.3186368, 0.26025128, -0.46153...  \n",
       "11911  [-0.23396452, -0.028989198, 0.41473043, -0.179...  \n",
       "11912  [0.22810979, 0.027687859, 0.12501115, -0.54732...  \n",
       "11913  [-0.41367865, 0.10774601, 0.24124433, -0.22921...  \n",
       "\n",
       "[11914 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_amazon_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(emb_amazon_corpus, 'emb_amazon_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_amazon_corpus = pd.read_pickle('emb_amazon_corpus.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
